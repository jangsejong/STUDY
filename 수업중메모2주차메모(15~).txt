11/29
선형회귀법(Linear Regression) : 
단순 선형 회귀 _ simple linear regression
다중 선형 회귀 _ multiple linear regression

시그모이드함수 (sigmoid) : logit과 sigmoid는 역함수 관계
                        2개의 클래스로 정의하던 logit을 K 개의 클래스로 확장하면 softmax
                        Sigmoid를 K 개 클래스를 대상으로 일반화하면 softmax, softmax에서 K=2로 두면 sigmoid로 환원
                        소프트맥스와 시그모이드의 가장 큰 차이는 값 개수이다.
                        소프트맥스는 다분류 출력이 가능하다
                        그러나 시그모이드는 이진분류의 경우 사용하며 실수 값 기준으로 (0.5이상 or not) 0이나 1로 출력한다
                        링크 : https://blog.naver.com/th9231/221989350922
                        그리고 용도가 소프트맥스는 보통 출력층(classification)에 사용하고
                        시그모이드는 중간에 activation function으로 사용된다.
이진분류의 activation = Sigmoid
                Loss = Binary Cross Entropy                       

부동소수점연산 : 부동 소수점 수 사이에서 일어나는 사칙 연산
                floating point operation의 약어. 
                매우 큰 수 또는 작은 수를 나타내거나 그러한 수를 높은 정밀도로 취급하기 위해 부동 소수점 수를 이용한 연산. 
                소수점의 위치를 컴퓨터 내부에서 자동적으로 조정하게 되므로, 프로그램에서는 소수점의 위치를 맞출 필요가 없다는 장점이 있어 복잡한 과학 기술 계산에 많이 사용된다.

디폴트 (default) : 응용 프로그램에서 사용자가 별도의 명령을 내리지 않았을 때, 시스템이 미리 정해진 값이나 조건을 자동으로 적용시키는 것. 
                운영 체계(OS)의 명령어들은 대부분 많은 종류의 매개 변수나 선택 사항을 지정해야 하는데, 
                이러한 매개 변수나 선택 사항을 사용자가 생략하면 시스템이 가장 자주 쓰이는 것으로 설정하거나 명령어의 의미로부터 적절한 값을 선택하는 경우가 많다. 
                워드 프로세서에서 별도로 지정하지 않는 한 종이의 여백이 미리 정해진 규격으로 설정되는 것 등이 그 예이다.

binary crossentropy :
만약 이진 분류기를 훈련하려면, binary crossentropy 손실함수를 사용하면 됩니다. 
이진 분류기라는 것은 True 또는 False, 양성 또는 음성 등 2개의 클래스를 분류할 수 있는 분류기를 의미합니다. 

loss = cost = 손실 = 평가지표
==================================
1. categorical_crossentropy
다중 분류 손실 함수
출력 값이 one-hot encoding된 결과로 나온다. -> label(y)을 one-hot encoding해서 넣어줘야 함
클래스가 상호 배타적일 경우(e.g. 각 샘플이 정확히 하나의 클래스에 속하는 경우) 사용
 

2. sparse_categorical_crossentropy
다중 분류 손실 함수
integer type 클래스 -> one-hot encoding하지 않고 정수 형태로 label(y)을 넣어줌
한 샘플에 여러 클래스가 있거나 label이 soft 확률일 경우 사용
 

3. binary_crossentropy
binary 다중 분류 손실 함수
label들이 독립적일 때 사용
-----------
원-핫 인코딩 (One-Hot Encoding)
관련 링크 : https://wikidocs.net/22647

12/01
# 데이터 스케일링 (Data scaling)
minmax scaler : x값을 0~1 사이로 변환
과적합을 해결하기 위해서  train 비율 만큼 test 도 변환하여 predict 과적합을 방지한다
link : https://homeproject.tistory.com/3
     : https://wooono.tistory.com/96

#scaler = MinMaxScaler()
#scaler = StandardScaler()
#scaler = RobustScaler()
#scaler = MaxAbsScaler()

scaler.fit(x_train)
x_train = scaler.transform(x_train)
x_test = scaler.transform(x_test)

#Standardization (표준화)
특성들의 평균을 0, 분산을 1 로 스케일링하는 것입니다.
즉, 특성들을 정규분포로 만드는 것입니다

#Normalization (정규화)
특성들을 특정 범위(주로 [0,1]) 로 스케일링 하는 것입니다.
가작 작은 값은 0, 가장 큰 값은 1 로 변환되므로, 모든 특성들은 [0, 1] 범위를 갖게됩니다.

scaler 는 fit 과 transform 메서드를 지니고 있습니다.
fit 메서드는 훈련 데이터에만 적용해, 훈련 데이터의 분포를 먼저 학습하고
그 이후, transform 메서드를 훈련 데이터와 테스트 데이터에 적용해 스케일을 조정해야합니다.
따라서, 훈련 데이터에는 fit_transform() 메서드를 적용하고, 테스트 데이터에는 transform() 메서드를 적용해야합니다.
fit_transform() 은 fit 과 transform 이 결합된 단축 메서드입니다.

#StandardScaler()
특성들의 평균을 0, 분산을 1 로 스케일링하는 것입니다.
즉, 특성들을 정규분포로 만드는 것입니다.
최솟값과 최댓값의 크기를 제한하지 않기 때문에, 어떤 알고리즘에서는 문제가 있을 수 있으며
이상치에 매우 민감합니다.
회귀보다 분류에 유용합니다.

#MinMaxScaler()
Min-Max Normalization 이라고도 불리며,
특성들을 특정 범위(주로 [0,1]) 로 스케일링 하는 것입니다.
가작 작은 값은 0, 가장 큰 값은 1 로 변환되므로, 모든 특성들은 [0, 1] 범위를 갖게됩니다.
이상치에 매우 민감합니다.
분류보다 회귀에 유용합니다.

#MaxAbsScaler()
각 특성의 절대값이 0 과 1 사이가 되도록 스케일링합니다.
즉, 모든 값은 -1 과 1 사이로 표현되며, 데이터가 양수일 경우 MinMaxScaler 와 같습니다.
이상치에 매우 민감합니다.

#RobustScaler()
평균과 분산 대신에 중간 값과 사분위 값을 사용합니다.
중간 값은 정렬시 중간에 있는 값을 의미하고
사분위값은 1/4, 3/4에 위치한 값을 의미합니다.
이상치 영향을 최소화할 수 있습니다.

#Normalizer()
앞의 4가지 스케일러는 각 특성(열)의 통계치를 이용하여 진행됩니다.
그러나 Normalizer 의 경우 각 샘플(행)마다 적용되는 방식입니다.
이는 한 행의 모든 특성들 사이의 유클리드 거리(L2 norm)가 1이 되도록 스케일링합니다.
일반적인 데이터 전처리의 상황에서 사용되는 것이 아니라
모델(특히나 딥러닝) 내 학습 벡터에 적용하며,
특히나 피쳐들이 다른 단위(키, 나이, 소득 등)라면 더더욱 사용하지 않습니다.

# summary - model.summary()
: 연산개수확인  , 바이어스로 인해 각 레이어마다 노드 1개가 더 있는것처럼 연산 갯수가 늘어난다.
